{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Homework 1 Part B </h2>\n",
    "<i> Gawan Dekker 11025654, Marvin Lau 12364282, Bobbie van Gorp 11161108</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Implementation including documentation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "from random import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, sqrt\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Generate Input </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_input_unsorted(length, n):\n",
    "    \"\"\"Creates a list of all possible combinations of relevance scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "        Length of a combination of relevance scores.\n",
    "    n : int\n",
    "        Maximum relevance score\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A list containing all possible combinations of relevance scores.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    out.append([0] * length)\n",
    "    for i in range(length):\n",
    "        for j in range(1, n):\n",
    "            affix = [0] * i + [j]\n",
    "            suffixes = gen_input_unsorted(length - (i + 1), n)\n",
    "            for suffix in suffixes:\n",
    "                out.append(affix + suffix)\n",
    "    return out\n",
    "\n",
    "def gen_input(length, n):\n",
    "    \"\"\"Creates a sorted list of all possible combinations\n",
    "    of relevance scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "        Length of a combination of relevance scores.\n",
    "    n : int\n",
    "        Maximum relevance score\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A sorted list containing all possible combinations\n",
    "        of relevance scores.\n",
    "    \"\"\"\n",
    "    if length > 0:\n",
    "        out = gen_input_unsorted(length, n)\n",
    "        out.sort()\n",
    "    else:\n",
    "        out = []\n",
    "    return out\n",
    "\n",
    "def gen_input_pairs(length, n):\n",
    "    \"\"\"Creates a sorted list of all possible pairs of combinations\n",
    "    of relevance scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "        Length of a combination of relevance scores.\n",
    "    n : int\n",
    "        Maximum relevance score\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A sorted list containing all possible pairs of combinations\n",
    "        of relevance scores.\n",
    "    \"\"\"\n",
    "    out = gen_input(length * 2, n)\n",
    "    for i in range(len(out)):\n",
    "        out[i] = (out[i][:length], out[i][length:])\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_conflicts(n, length, _in=[], ordered=False):\n",
    "    \"\"\"Creates a list of all possible id conflicts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Total possible number of id conflicts at a time.\n",
    "    length: int\n",
    "        Length of the list containing id conflicts.\n",
    "    _in : array_like\n",
    "        Array of current id conflicts.\n",
    "    ordered : bool\n",
    "        Indicates whether id conflict numbers are to appear in order\n",
    "        or are permitted to appear in any order.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A list containing all possible id conflicts.\n",
    "        A value of 0 indicates no conflict,\n",
    "        a value higher than indicates a conflict.\n",
    "    \"\"\"\n",
    "    if (len(_in) < length and n <= length):\n",
    "        out = []\n",
    "        # Add all possible conflicts.\n",
    "        for i in [x for x in range(1, n + 1) if x not in _in]:\n",
    "            out += get_conflicts(n, length, _in + [i], ordered)\n",
    "            if ordered:\n",
    "                break\n",
    "        # Add absence of a conflict if possible.\n",
    "        if (length - len(_in) > n - sum([1 for i in _in if i != 0])):\n",
    "            out += get_conflicts(n, length, _in + [0], ordered)\n",
    "        return out\n",
    "    else:\n",
    "        return [_in]\n",
    "\n",
    "def add_conflicts(pair):\n",
    "    \"\"\"Adds id conflicts to a tuple of combinations of relevance grades.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pair : tuple\n",
    "        Tuple of combinations of relevance grades.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A list of all possible tuples of combinations\n",
    "        of relevance grades with id conflicts.\n",
    "        Relevance grades from the inputs have been replaced by a tuple\n",
    "        of relevance grade and id conflict number.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    # Add all possible id conflicts to input pair.\n",
    "    for n in range(len(pair[0]) + 1):\n",
    "        for ids0 in get_conflicts(n, len(pair[0]), ordered=True):\n",
    "            ranking0 = list(zip(pair[0], ids0))\n",
    "            for ids1 in get_conflicts(n, len(pair[0])):\n",
    "                out.append((ranking0, list(zip(pair[1], ids1))))\n",
    "    # Remove any pairs in which conflicts appear\n",
    "    # where relevance grades do not match.\n",
    "    for i in range(len(out) - 1, -1, -1):\n",
    "        delete = False\n",
    "        for r0, id0 in out[i][0]:\n",
    "            if id0 > 0:\n",
    "                for r1, id1 in out[i][1]:\n",
    "                    if id0 == id1 and not r0 == r1:\n",
    "                        delete = True\n",
    "                        break\n",
    "            if delete:\n",
    "                out.pop(i)\n",
    "                break\n",
    "    return out\n",
    "\n",
    "\n",
    "def ERR(g_list, R_func=lambda g, max_g: float(2**g- 1) / 2**max_g):\n",
    "    \"\"\"Calculates Expected Reciprocal Rank for one list\n",
    "    of relevance grades.\n",
    "    \n",
    "    Source\n",
    "    ------\n",
    "    The algorithm originates from a paper by O. Chapelle et al.\n",
    "    named \"Expected Reciprocal Rank for Graded Relevance\"\n",
    "    and can be found here: http://olivier.chapelle.cc/pub/err.pdf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    g_list : array_like\n",
    "        Array of relevance grades.\n",
    "    R_func : function(int, int) -> float\n",
    "        Function that converts a relevance grade\n",
    "        to probability of relevance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ERR : float\n",
    "        Expected Reciprocal Rank.\n",
    "    \"\"\"\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "    max_g = max(g_list)\n",
    "    for r in range(1, len(g_list) + 1):\n",
    "        R = R_func(g_list[r - 1], max_g)\n",
    "        ERR += p * R / float(r)\n",
    "        p *= 1 - R\n",
    "    return ERR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Interleaving </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_interleaving(ranking_pair,max_interleav=3):\n",
    "    \"\"\"Run Team-draft interleaving given a ranking pair as input\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    An ranking pair : List of ranked and labeled results\n",
    "        Index of list elements represents the rank of that element. Each\n",
    "        element is a tuple of the relevance score and a duplicate ID.\n",
    "        If this value is 0, it has no duplicate with a document in the results of other ranker.\n",
    "        If the value is greater than 0, then it has a duplicate with another result of the other ranker that matches this number\n",
    "        No duplicate example: E ranked list:  [(0,0),(0,0),(0,0)] and P ranked list: [(1,0),(0,0),(0,0)] form\n",
    "        2 duplicates example: E ranked list: [(1,1),(0,2),(0,0)]  and P ranked list: [(1,0),(1,1),(0,2)]  for example\n",
    "    Returns\n",
    "    -------\n",
    "    Interleaved list (of length 3 as default) based on Team-draft method: list\n",
    "        Index + 1 represents the rank of the interleaved list and element is an tuple of the form (relevance: binary,ranker credit:binary), Credits are assigned as P(0) and E(1)\n",
    "    \"\"\"\n",
    "    ranking_p = ranking_pair[0] #[(0,0),(0,0),(0,0)] form or duplicate [(0,1),(0,0),(0,0)]\n",
    "    ranking_e = ranking_pair[1]\n",
    "    interleaved = []\n",
    "\n",
    "    p_team = 0 #Amount results assigned from p\n",
    "    e_team = 0\n",
    "    p_pointer = 0 #Next top result from ranking p\n",
    "    e_pointer = 0\n",
    "    found_duplicates = [] #Duplicate documents have an ID of greater than 0. A matching number is an duplciate\n",
    "    limit = len(ranking_p)\n",
    "\n",
    "    #while p_pointer < limit and e_pointer < limit:\n",
    "    while len(interleaved) < max_interleav:\n",
    "\n",
    "        p_priority = np.random.choice(2, 1)[0]\n",
    "        new_result = False\n",
    "        if (p_team < e_team) or (p_team == e_team and p_priority == 1):\n",
    "\n",
    "            while not new_result:\n",
    "                top_result_p = ranking_p[p_pointer]\n",
    "                relevance_p, duplicate_id_p = top_result_p\n",
    "\n",
    "                p_pointer += 1\n",
    "                if duplicate_id_p not in found_duplicates:\n",
    "                    new_result = True\n",
    "                    break\n",
    "                elif p_pointer == limit:\n",
    "                    break\n",
    "\n",
    "            if new_result:\n",
    "                #interleaved.append( (relevance_p,\"P\") )\n",
    "                interleaved.append((relevance_p, 0))\n",
    "                p_team += 1\n",
    "\n",
    "                if duplicate_id_p > 0:\n",
    "                    found_duplicates.append(duplicate_id_p)\n",
    "        else:\n",
    "\n",
    "            while not new_result:\n",
    "                top_result_e = ranking_e[e_pointer]\n",
    "                relevance_e, duplicate_id_e = top_result_e\n",
    "                e_pointer += 1\n",
    "\n",
    "                if duplicate_id_e not in found_duplicates:\n",
    "                    new_result = True\n",
    "                    break\n",
    "                elif e_pointer == limit:\n",
    "                    break\n",
    "\n",
    "            if new_result:\n",
    "                #interleaved.append((relevance_e, \"E\"))\n",
    "                interleaved.append((relevance_e, 1))\n",
    "                e_team += 1\n",
    "\n",
    "                if duplicate_id_e > 0:\n",
    "                    found_duplicates.append(duplicate_id_e)\n",
    "\n",
    "    return interleaved\n",
    "\n",
    "def get_softmax(ranking_indices,tau):\n",
    "    \"\"\"Compute softmax distribution for a ranker given the indices of documents that are avaliable to be picked.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Ranking indices : List of indices that represent the rank of documents that can be picked for next interleaving.\n",
    "    Example of nothing picked of ranker: List of the form: [0,1,2]\n",
    "    Example of document already picked of ranker: List of the form: [0,2]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of softmax probabilities for documents to be picked: List\n",
    "        The list maps one to one the ranking indices that were given as input, each element is a probability for the document for that particular index\n",
    "        in that ranking indices list, which in turn consist of numbers that represents the actual indices to be picked from ranked results.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    numerator_list = [] #Numerator values for each of the ranked results\n",
    "    softmax_distribution = []\n",
    "\n",
    "    for rank_index in ranking_indices:\n",
    "        rank = rank_index + 1\n",
    "        numerator_value = 1/(rank**tau)\n",
    "        numerator_list.append(numerator_value)\n",
    "\n",
    "    denominator = sum(numerator_list)\n",
    "\n",
    "    for value in numerator_list:\n",
    "        probability = value/denominator\n",
    "        softmax_distribution.append(probability)\n",
    "\n",
    "    return softmax_distribution\n",
    "\n",
    "def prob_interleaving(ranking_pair,max_interleav=3,tau=3):\n",
    "    \"\"\"Run Probabilistic interleaving  given a ranking pair as input\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    An ranking pair : List of ranked and labeled results\n",
    "        Index of list elements represents the rank of that element. Each\n",
    "        element is a tuple of the relevance score and a duplicate ID.\n",
    "        If this value is 0, it has no duplicate with a document in the results of other ranker.\n",
    "        If the value is greater than 0, then it has a duplicate with another result of the other ranker that matches this number\n",
    "        No duplicate example: E ranked list:  [(0,0),(0,0),(0,0)] and P ranked list: [(1,0),(0,0),(0,0)] form\n",
    "        2 duplicates example: E ranked list: [(1,1),(0,2),(0,0)]  and P ranked list: [(1,0),(1,1),(0,2)]  for example\n",
    "    Returns\n",
    "    -------\n",
    "    Interleaved list (of length 3 as default) based on probabilistc interleaving method: list\n",
    "        Index + 1 represents the rank of the interleaved list and element is an tuple of the form (relevance: binary,ranker credit:binary), credits are assigned as P(0) and E(1)\n",
    "    \"\"\"\n",
    "\n",
    "    ranking_p = ranking_pair[0] #[(0,0),(0,0),(0,0)] form or duplicate [(0,1),(0,0),(0,0)]\n",
    "    ranking_e = ranking_pair[1] #[(0,0),(0,0),(0,0)] form or duplicate #[(0,1),(0,0),(0,0)]\n",
    "    interleaved = []\n",
    "    limit = len(ranking_p)\n",
    "\n",
    "    p_indices = list(range(limit))\n",
    "    e_indices = list(range(limit))\n",
    "\n",
    "    found_duplicates = []\n",
    "\n",
    "    #while len(p_indices) > 0 or len(e_indices) > 0:\n",
    "    while len(interleaved) < max_interleav:\n",
    "\n",
    "        p_priority = np.random.choice(2, 1)[0]\n",
    "\n",
    "        if (p_priority and len(p_indices) > 0) or len(e_indices) == 0:\n",
    "            softmax_p = get_softmax(p_indices,tau)\n",
    "            doc_index_p = np.random.choice(p_indices, 1, p=softmax_p)[0]\n",
    "            p_indices.remove(doc_index_p)\n",
    "\n",
    "            result_p = ranking_p[doc_index_p]\n",
    "            relevance_p, duplicate_id_p = result_p\n",
    "\n",
    "            if duplicate_id_p == 0 :\n",
    "                #interleaved.append((relevance_p, \"P\"))\n",
    "                interleaved.append((relevance_p, 0))\n",
    "            elif (duplicate_id_p > 0 and duplicate_id_p not in found_duplicates):\n",
    "                #interleaved.append((relevance_p, \"P\"))\n",
    "                interleaved.append((relevance_p, 0))\n",
    "                found_duplicates.append(duplicate_id_p)\n",
    "                duplicate_index = ranking_e.index(result_p)\n",
    "                e_indices.remove(duplicate_index)\n",
    "        else:\n",
    "            softmax_e = get_softmax(e_indices,tau)\n",
    "            doc_index_e = np.random.choice(e_indices, 1, p=softmax_e)[0]\n",
    "            e_indices.remove(doc_index_e)\n",
    "\n",
    "            result_e = ranking_e[doc_index_e]\n",
    "            relevance_e, duplicate_id_e = result_e\n",
    "\n",
    "            if duplicate_id_e == 0:\n",
    "                #interleaved.append((relevance_e, \"E\"))\n",
    "                interleaved.append((relevance_e, 1))\n",
    "            elif (duplicate_id_e > 0 and duplicate_id_e not in found_duplicates):\n",
    "                #interleaved.append((relevance_e, \"E\"))\n",
    "                interleaved.append((relevance_e, 1))\n",
    "                found_duplicates.append(duplicate_id_e)\n",
    "                duplicate_index = ranking_p.index(result_e)\n",
    "                p_indices.remove(duplicate_index)\n",
    "\n",
    "    return interleaved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Click Models </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_yandex(path, n=-1):\n",
    "    \"\"\"Reads yandex database.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to database file.\n",
    "    n : int\n",
    "        Number of lines to read. Is ignored if value is lower than 0.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A list of dictionaries representing database entries.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    with open(path) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            if n >= 0 and i > n:\n",
    "                break\n",
    "            data = l.strip().split('\\t')\n",
    "            item = {\n",
    "                'id'   : int(data[0]),\n",
    "                't'    : int(data[1]),\n",
    "                'a'    : data[2].lower(),\n",
    "                'a_id' : int(data[3])\n",
    "            }\n",
    "            if item['a'] == 'q':\n",
    "                item['r_id'] = int(data[4])\n",
    "                item['urls'] = [int(x) for x in data[5:]]\n",
    "            out.append(item)\n",
    "    return out\n",
    "\n",
    "\n",
    "class RCM:\n",
    "    \"\"\"Random Click Model\n",
    "    ==================\n",
    "    \n",
    "    Click model that simulates user interaction as random clicking.\n",
    "    \n",
    "    One parameter `rho` is learned signifying the chance\n",
    "    of a document being clicked. User interaction is then simulated\n",
    "    by comparing a random value drawn from a uniform distribution\n",
    "    against this `rho` value.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes class parameters.\n",
    "        \"\"\"\n",
    "        self.rho = random()\n",
    "    \n",
    "    def learn(self, database, n=-1):\n",
    "        \"\"\"Learns class parameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        database : array_like\n",
    "            Array of dictionaries representing database items.\n",
    "        n : int\n",
    "            Maximum rank at which parameters are learned.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        n_clicks, n_docs = 0, 0\n",
    "        for item in database:\n",
    "            if item['a'] == 'q':\n",
    "                if n < 0:\n",
    "                    n_docs += len(item['urls'])\n",
    "                else:\n",
    "                    n_docs += len(item['urls'][:n])\n",
    "            else:\n",
    "                n_clicks += 1\n",
    "        self.rho = n_clicks / float(n_docs)\n",
    "        return\n",
    "    \n",
    "    def get_p(self, relevance_grades):\n",
    "        \"\"\"Determines chance of clicking on a document.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        relevance_grades : array_like\n",
    "            Array containing relevance grades for all documents\n",
    "            returned by a search query.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : list\n",
    "            List of probabilities corresponding\n",
    "            to entries in `search_results`.\n",
    "        \"\"\"\n",
    "        out = [self.rho for _ in range(len(relevance_grades))]\n",
    "        return out\n",
    "    \n",
    "    def get_clicks(self, relevance_grades):\n",
    "        \"\"\"Simulate user interaction by determining\n",
    "        what documents are clicked on.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        relevance_grades : array_like\n",
    "            Array containing relevance grades for all documents\n",
    "            returned by a search query.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : list\n",
    "            List of indices of the documents that were clicked on\n",
    "            in the simulation.\n",
    "        \"\"\"\n",
    "        p = self.get_p(relevance_grades)\n",
    "        out = []\n",
    "        for i in range(len(p)):\n",
    "            if random() <= p[i]:\n",
    "                out.append(i)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PBM:\n",
    "    \"\"\"Position Based Click Model\n",
    "    ==========================\n",
    "    \n",
    "    Click model that simulates user interaction\n",
    "    based on rank and document score.\n",
    "    \n",
    "    Two sets of parameters are learned, `alphas` and `gammas`.\n",
    "    Parameters in `alphas` represent the attractiveness of documents\n",
    "    given a certain query, while parameters in `gammas` represent\n",
    "    the chance of viewing a document at a specific rank.\n",
    "    User interaction is simulated by comparing a random value\n",
    "    drawn from a uniform distribution against the product of\n",
    "    one of the parameters in `gammas` and a value `epsilon`.\n",
    "    Here `epsilon` represents the chance of clicking on a document\n",
    "    even though the document is irrelevant and vice versa (`epsilon` replaces\n",
    "    the parameters in `alpha` due to data sparcity).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes class parameters.\n",
    "        \"\"\"\n",
    "        self.alphas = {}\n",
    "        self.gammas = []\n",
    "    \n",
    "    def update(self, item, alpha_sum, gamma_sum, prev_q, clicks, n=-1):\n",
    "        \"\"\"Updates sum of alpha and gamma contributions\n",
    "        for the previous query database item.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        item : dict\n",
    "            A dictionary representing a database item.\n",
    "        alpha_sum : dict\n",
    "            A dictionary containing the summed contributions and\n",
    "            number of contributions for all alphas corresponding\n",
    "            to a document and query pair.\n",
    "        gamma_sum : array_like\n",
    "            A list containing the summed contributions of all gammas\n",
    "            corresponding to a rank.\n",
    "        prev_q : dict\n",
    "            A dictionary representing the previous query database item.\n",
    "        clicks : array_like\n",
    "            A list of documents ids, returned by the last query,\n",
    "            that the user clicked on.\n",
    "        n : int\n",
    "            Maximum rank at which parameters are learned.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        alpha_sum : dict\n",
    "            A dictionary containing the summed contributions and\n",
    "            number of contributions for all parameters in `alphas`\n",
    "            corresponding to a document and query pair.\n",
    "        gamma_sum : array_like\n",
    "            A list containing the summed contributions for all\n",
    "            parameters in `gammas` corresponding to a rank.\n",
    "        prev_q : dict\n",
    "            A dictionary representing the previous query database item.\n",
    "        clicks : array_like\n",
    "            A list of documents ids, returned by the last query,\n",
    "            that the user clicked on.\n",
    "        \"\"\"\n",
    "        if item['a'] == 'q':\n",
    "            q_id = prev_q['a_id']\n",
    "            q_urls = prev_q['urls']\n",
    "            if n >= 0:\n",
    "                q_urls = q_urls[:n]\n",
    "            gamma_length = len(q_urls)\n",
    "            # Extend gammas and gamma_sum.\n",
    "            while len(self.gammas) < gamma_length:\n",
    "                self.gammas.append(random())\n",
    "            while len(gamma_sum) < gamma_length:\n",
    "                gamma_sum.append(0)\n",
    "            for r, _id in enumerate(q_urls):\n",
    "                uq = str((_id, q_id))\n",
    "                # Extend alphas and alpha_sum.\n",
    "                if self.alphas.get(uq) == None:\n",
    "                    self.alphas[uq] = random()\n",
    "                if alpha_sum.get(uq) == None:\n",
    "                    alpha_sum[uq] = {'sum' : 0, 'length' : 0}\n",
    "                # Update alphs_sum and gamma_sum.\n",
    "                if _id in clicks:\n",
    "                    alpha_sum[uq]['sum'] += 1\n",
    "                    gamma_sum[r] += 1\n",
    "                else:\n",
    "                    alpha_sum[uq]['sum'] += \\\n",
    "                        (1 - self.gammas[r]) * self.alphas[uq] \\\n",
    "                        / (1 - self.gammas[r] * self.alphas[uq])\n",
    "                    gamma_sum[r] += \\\n",
    "                        self.gammas[r] * (1 - self.alphas[uq]) \\\n",
    "                        / (1 - self.gammas[r] * self.alphas[uq])\n",
    "                alpha_sum[uq]['length'] += 1\n",
    "            prev_q = item\n",
    "            clicks = []\n",
    "        else:\n",
    "            # Record clicked document id.\n",
    "            clicks.append(item['a_id'])\n",
    "        return alpha_sum, gamma_sum, prev_q, clicks\n",
    "    \n",
    "    def _learn(self, database, n=-1):\n",
    "        \"\"\"Learns class parameters for one run over the given database.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        database : array_like\n",
    "            Array of dictionaries representing database items.\n",
    "        n : int\n",
    "            Maximum rank at which parameters are learned.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        alpha_sum = {}\n",
    "        gamma_sum = []\n",
    "        prev_q = database[0]\n",
    "        session_id = prev_q['id']\n",
    "        clicks = []\n",
    "        \n",
    "        empty_q = {'id' : -1, 'a' : 'q', 'a_id' : -1, 'urls' : []}\n",
    "        \n",
    "        # Adjust query counter for empty query.\n",
    "        n_queries = 0\n",
    "        if prev_q['a'] != 'q':\n",
    "            n_queries -= 1\n",
    "        \n",
    "        # Sum alpha and gamma contributions\n",
    "        # of each item in the database.\n",
    "        for item in database[1:] + [empty_q]:\n",
    "            if item['a'] == 'q':\n",
    "                n_queries += 1\n",
    "            if session_id != item['id']:\n",
    "                alpha_sum, gamma_sum, prev_q, clicks = self.update(\n",
    "                    empty_q, alpha_sum, gamma_sum, prev_q, clicks, n)\n",
    "                session_id = item['id']\n",
    "            alpha_sum, gamma_sum, prev_q, clicks = self.update(\n",
    "                item, alpha_sum, gamma_sum, prev_q, clicks, n)\n",
    "        \n",
    "        # Update alphas and gammas.\n",
    "        for uq, alpha in alpha_sum.items():\n",
    "            self.alphas[uq] = (alpha['sum'] + 1) \\\n",
    "                / float(alpha['length'] + 2)\n",
    "        for r, gamma_sum_r in enumerate(gamma_sum):\n",
    "            self.gammas[r] = (gamma_sum_r + 1) / float(n_queries + 1)\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def learn(self, database, n_decimals, n_consecutive, n_rank=-1):\n",
    "        \"\"\"Learns class parameters on the given database\n",
    "        until convergence.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        database : array_like\n",
    "            Array of dictionaries representing database items.\n",
    "        n_decimals : int\n",
    "            Number of decimals on which convergence is checked.\n",
    "        n_consecutive : int\n",
    "            Number of consecutive database iterations for which\n",
    "            convergence is checked.\n",
    "        n_rank : int\n",
    "            Maximum rank at which parameters are learned.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        prev_gammas = []\n",
    "        convergence = False\n",
    "        while convergence == False:\n",
    "            self._learn(database, n_rank)\n",
    "            prev_gammas.append(\n",
    "                [round(gamma, n_decimals) for gamma in self.gammas])\n",
    "            if len(prev_gammas) >= n_consecutive:\n",
    "                while len(prev_gammas) > n_consecutive:\n",
    "                    prev_gammas.pop(0)\n",
    "                convergence = True\n",
    "                for prev, cur in zip(prev_gammas[:-1], prev_gammas[1:]):\n",
    "                    for gamma_prev, gamma_cur in zip(prev, cur):\n",
    "                        if gamma_prev - gamma_cur != 0:\n",
    "                            convergence = False\n",
    "                            break\n",
    "                    if convergence == False:\n",
    "                        break\n",
    "        return\n",
    "    \n",
    "    def get_p(self, relevance_grades, epsilon=1e-1):\n",
    "        \"\"\"Determines chance of clicking on a document.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        relevance_grades : array_like\n",
    "            Array containing relevance grades for all documents\n",
    "            returned by a search query.\n",
    "        epsilon : float\n",
    "            Value representing the chance of clicking on a document\n",
    "            even though the document is irrelevant and vice versa.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : list\n",
    "            List of probabilities corresponding\n",
    "            to entries in `search_results`.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "        for i in range(min(len(self.gammas), len(relevance_grades))):\n",
    "            if relevance_grades[i] == 0:\n",
    "                out.append(self.gammas[i] * epsilon)\n",
    "            else:\n",
    "                out.append(self.gammas[i] * (1 - epsilon))\n",
    "        return out\n",
    "    \n",
    "    def get_clicks(self, relevance_grades, epsilon=1e-1):\n",
    "        \"\"\"Simulate user interaction by determining\n",
    "        what documents are clicked on.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        relevance_grades : array_like\n",
    "            Array containing relevance grades for all documents\n",
    "            returned by a search query.\n",
    "        epsilon : float\n",
    "            Value representing the chance of clicking on a document\n",
    "            even though the document is irrelevant and vice versa.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        out : list\n",
    "            List of indices of the documents that were clicked on\n",
    "            in the simulation.\n",
    "        \"\"\"\n",
    "        p = self.get_p(relevance_grades)\n",
    "        out = []\n",
    "        for i in range(len(p)):\n",
    "            if random() <= p[i]:\n",
    "                out.append(i)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Power Analysis </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interleaving_simulation(pair, k, interleaving_func, click_model_func, length_interleaving=-1):\n",
    "    \"\"\"Simulates user interaction on interleaved search results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pair : tuple\n",
    "        Pair of ranking combinations.\n",
    "    k : int\n",
    "        Number of simulations.\n",
    "    interleaving_func : function(tuple) -> list\n",
    "        Function to interleave the two lists of ranking combinations.\n",
    "    click_model : function(array_like) -> int\n",
    "        Function to simulate user click.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    p : float\n",
    "        Proportion of wins of second ranking combination in pair.\n",
    "    \"\"\"\n",
    "    wins = [0] * len(pair)\n",
    "    for _ in range(k):\n",
    "        # Create interleaved list\n",
    "        search_results = interleaving_func(pair, length_interleaving)\n",
    "        # Get relevance label of documents in interleaved list\n",
    "        relevance_grades = []\n",
    "        for relevance, assignment in search_results:\n",
    "            relevance_grades.append(relevance)\n",
    "        # Get clicked documents indices from interleaved list\n",
    "        clicked = click_model_func(relevance_grades)\n",
    "        n_E_click = 0\n",
    "        n_P_click = 0\n",
    "        # Determine who got most clicks\n",
    "        for click in clicked:\n",
    "            assignment = search_results[click][1]\n",
    "            if assignment == 1:\n",
    "                n_E_click += 1\n",
    "            else:\n",
    "                n_P_click += 1\n",
    "\n",
    "        if n_E_click == n_P_click:\n",
    "            wins[0] += 1\n",
    "            wins[1] += 1\n",
    "        elif n_E_click > n_P_click:\n",
    "            wins[1] += 1\n",
    "        else:\n",
    "            wins[0] += 1\n",
    "\n",
    "    p = wins[1] / float(wins[0] + wins[1])\n",
    "    return p\n",
    "\n",
    "\n",
    "def compute_sample_size(p1, alpha=0.05, beta=0.10):\n",
    "    \"\"\"Computes sample size for a given proportion\n",
    "    based on power analysis. Returns -1 if p1 == 0.5.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        p1 : float\n",
    "            Proportion for which sample size is to be calculated.\n",
    "        alpha : float\n",
    "            Type I error parameter.\n",
    "        beta : float\n",
    "            Type II error parameter.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        n : int\n",
    "            Sample size.\n",
    "    \"\"\"\n",
    "    p0 = 0.5\n",
    "    # Return -1 if given proportion is exactly 50%.\n",
    "    diff = p1 - p0\n",
    "    if diff == 0:\n",
    "        return -1\n",
    "    # Compute sample size.\n",
    "    z_alpha = stats.norm.ppf(1 - alpha)\n",
    "    z_beta = stats.norm.ppf(1 - beta)\n",
    "    sigma0 = sqrt(p0 * (1 - p0))\n",
    "    sigma1 = sqrt(p1 * (1 - p1))\n",
    "    n = ((z_alpha * sigma0 + z_beta * sigma1) / diff) ** 2\n",
    "    return ceil(n)\n",
    "\n",
    "\n",
    "def get_bin_labels(n_bins, n_decimals=3, cut_sides=0.0):\n",
    "    \"\"\"Creates labels containing ranges for bins.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_bins : int\n",
    "        Amount of bins to label.\n",
    "    n_decimals : int\n",
    "        Number of decimals to round the ranges on.\n",
    "    cut_sides : float\n",
    "        Amount by which first and last bin have their ranges cut.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A list containing labels.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    step = 1.0 / n_bins\n",
    "    for i in range(n_bins):\n",
    "        # Determine delta range of bin.\n",
    "        dmin = i * step\n",
    "        dmax = dmin + step\n",
    "        if i == 0:\n",
    "            dmin += cut_sides\n",
    "        if i == n_bins - 1:\n",
    "            dmax -= cut_sides\n",
    "        dmin = round(dmin, n_decimals)\n",
    "        dmax = round(dmax, n_decimals)\n",
    "        out.append('[' + str(dmin) + ' - ' + str(dmax) + ')')\n",
    "    return out\n",
    "\n",
    "def process_bins(bins):\n",
    "    \"\"\"Determines minimum value, maximum value and median\n",
    "    for each of the given bins.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bins : array_like\n",
    "        Array of bins containing numerical values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A list of dictionaries containing the minimum value,\n",
    "        maximum value and median for each bin.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for cur in bins:\n",
    "        if cur == []:\n",
    "            out.append({'has_info' : False})\n",
    "        else:\n",
    "            # Sort bin and remove error-data.\n",
    "            cur.sort()\n",
    "            cur.reverse()\n",
    "            while cur[-1] == -1:\n",
    "                cur.pop()\n",
    "            cur.reverse()\n",
    "            # Determine minimum, median and maximum.\n",
    "            d, m = divmod(len(cur), 2)\n",
    "            median = ceil((cur[d] + cur[-int(not bool(m))]) / 2.0)\n",
    "            out.append({'min' : min(cur), 'max' : max(cur),\n",
    "                'median' : median, 'has_info' : True})\n",
    "    return out\n",
    "\n",
    "def print_bin_info(bin_info, labels=None):\n",
    "    \"\"\"Prints minimum value, median and maximum value\n",
    "    for each of the given bin info dictionaries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bins : array_like\n",
    "        Array of dictionaries containing `min`, `max` and `median`\n",
    "        fields, also contains a `has_info` field which is to be turned\n",
    "        set `False` if not all other fields are present.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    if labels == None:\n",
    "        labels = list(range(len(bin_info)))\n",
    "    for info, label in zip(bin_info, labels):\n",
    "        print('BIN', label)\n",
    "        if info['has_info']:\n",
    "            print('     min', info['min'])\n",
    "            print('  median', info['median'])\n",
    "            print('     max', info['max'])\n",
    "        else:\n",
    "            print('  NO DATA')\n",
    "        print()\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_bin_info(bin_info_list, bin_set_labels=[], bin_labels=[]):\n",
    "    \"\"\"Plots min/median/max information for the bins\n",
    "    in the given bin sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bin_info_list : array_like\n",
    "        A list of sets of bins containing dictionaries\n",
    "        with min/median/max/has_info information.\n",
    "    bin_set_labels : array_like\n",
    "        A list of labels for the different sets of bins.\n",
    "    bin_labels : array_like\n",
    "        A list of labels of the different bins in the bin sets.\n",
    "        Is used to label the x axis.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    if bin_set_labels == []:\n",
    "        bin_set_labels = list(range(len(bin_info_list)))\n",
    "    # Plot median with min and max as error bars\n",
    "    # for each bin in each bin set.\n",
    "    for i, (bin_info, label) in enumerate(zip(bin_info_list, bin_set_labels)):\n",
    "        x = [j + 0.1 * (i + 1) for j in range(len(bin_info))]\n",
    "        y = [info['median'] if info['has_info'] else 0 for info in bin_info]\n",
    "        err = ([info['median'] - info['min'] if info['has_info'] else 0\n",
    "            for info in bin_info],\n",
    "            [info['max'] - info['median'] if info['has_info'] else 0\n",
    "            for info in bin_info])\n",
    "        plt.errorbar(x, y, err, label=label)\n",
    "    # Apply bin labels.\n",
    "    if bin_labels != []:\n",
    "        plt.xticks(range(len(bin_labels)), bin_labels + [''], rotation=30)\n",
    "    plt.legend()\n",
    "    plt.xlabel('$\\Delta$ERR')\n",
    "    plt.ylabel('sample size')\n",
    "    plt.title('Determined sample size')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Execution </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Analysis </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
