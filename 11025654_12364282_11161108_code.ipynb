{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Homework 1 Part B </h2>\n",
    "<i> Gawan Dekker 11025654, Marvin Lau 12364282, Bobbie van Gorp 11161108</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Implementation </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate input\n",
    "\n",
    "def gen_input_unsorted(length, n):\n",
    "    out = []\n",
    "    out.append([0] * length)\n",
    "    for i in range(length):\n",
    "        for j in range(1, n):\n",
    "            affix = [0] * i + [j]\n",
    "            suffixes = gen_input_unsorted(length - (i + 1), n)\n",
    "            for suffix in suffixes:\n",
    "                out.append(affix + suffix)\n",
    "    return out\n",
    "\n",
    "def gen_input(length, n):\n",
    "    if length > 0:\n",
    "        out = gen_input_unsorted(length, n)\n",
    "        out.sort()\n",
    "    else:\n",
    "        out = []\n",
    "    return out\n",
    "\n",
    "def gen_input_pairs(length, n):\n",
    "    out = gen_input(length * 2, n)\n",
    "    for i in range(len(out)):\n",
    "        out[i] = (out[i][:length], out[i][length:])\n",
    "    return out\n",
    "\n",
    "\n",
    "def get_conflicts(n, length, _in=[], ordered=False):\n",
    "    if (len(_in) < length and n <= length):\n",
    "        out = []\n",
    "        # Add all possible conflicts.\n",
    "        for i in [x for x in range(1, n + 1) if x not in _in]:\n",
    "            out += get_conflicts(n, length, _in + [i], ordered)\n",
    "            if ordered:\n",
    "                break\n",
    "        # Add absence of a conflict if possible.\n",
    "        if (length - len(_in) > n - sum([1 for i in _in if i != 0])):\n",
    "            out += get_conflicts(n, length, _in + [0], ordered)\n",
    "        return out\n",
    "    else:\n",
    "        return [_in]\n",
    "\n",
    "def add_conflicts(pair):\n",
    "    out = []\n",
    "    # Add all possible id conflicts to input pair.\n",
    "    for n in range(len(pair[0]) + 1):\n",
    "        for ids0 in get_conflicts(n, len(pair[0]), ordered=True):\n",
    "            ranking0 = list(zip(pair[0], ids0))\n",
    "            for ids1 in get_conflicts(n, len(pair[0])):\n",
    "                out.append((ranking0, list(zip(pair[1], ids1))))\n",
    "    # Remove any pairs in which conflicts appear\n",
    "    # where relevance grades do not match.\n",
    "    for i in range(len(out) - 1, -1, -1):\n",
    "        delete = False\n",
    "        for r0, id0 in out[i][0]:\n",
    "            if id0 > 0:\n",
    "                for r1, id1 in out[i][1]:\n",
    "                    if id0 == id1 and not r0 == r1:\n",
    "                        delete = True\n",
    "                        break\n",
    "            if delete:\n",
    "                out.pop(i)\n",
    "                break\n",
    "    return out\n",
    "\n",
    "\n",
    "def ERR(g_list, R_func=lambda g, max_g: float(2**g- 1) / 2**max_g):\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "    max_g = max(g_list)\n",
    "    for r in range(1, len(g_list) + 1):\n",
    "        R = R_func(g_list[r - 1], max_g)\n",
    "        ERR += p * R / float(r)\n",
    "        p *= 1 - R\n",
    "    return ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interleaving\n",
    "\n",
    "def td_interleaving(ranking_pair,max_interleav=3):\n",
    "    ranking_p = ranking_pair[0] #[(0,0),(0,0),(0,0)] form or duplicate [(0,1),(0,0),(0,0)]\n",
    "    ranking_e = ranking_pair[1]\n",
    "    interleaved = []\n",
    "\n",
    "    p_team = 0 #Amount results assigned from p\n",
    "    e_team = 0\n",
    "    p_pointer = 0 #Next top result from ranking p\n",
    "    e_pointer = 0\n",
    "    found_duplicates = [] #Duplicate documents have an ID of greater than 0. A matching number is an duplciate\n",
    "    limit = len(ranking_p)\n",
    "\n",
    "    while len(interleaved) < max_interleav:\n",
    "\n",
    "        p_priority = np.random.choice(2, 1)[0]\n",
    "        new_result = False\n",
    "        if (p_team < e_team) or (p_team == e_team and p_priority == 1):\n",
    "\n",
    "            while not new_result:\n",
    "                top_result_p = ranking_p[p_pointer]\n",
    "                relevance_p, duplicate_id_p = top_result_p\n",
    "\n",
    "                p_pointer += 1\n",
    "                if duplicate_id_p not in found_duplicates:\n",
    "                    new_result = True\n",
    "                    break\n",
    "                elif p_pointer == limit:\n",
    "                    break\n",
    "\n",
    "            if new_result:\n",
    "                interleaved.append((relevance_p, 0))\n",
    "                p_team += 1\n",
    "\n",
    "                if duplicate_id_p > 0:\n",
    "                    found_duplicates.append(duplicate_id_p)\n",
    "        else:\n",
    "\n",
    "            while not new_result:\n",
    "                top_result_e = ranking_e[e_pointer]\n",
    "                relevance_e, duplicate_id_e = top_result_e\n",
    "                e_pointer += 1\n",
    "\n",
    "                if duplicate_id_e not in found_duplicates:\n",
    "                    new_result = True\n",
    "                    break\n",
    "                elif e_pointer == limit:\n",
    "                    break\n",
    "\n",
    "            if new_result:\n",
    "                interleaved.append((relevance_e, 1))\n",
    "                e_team += 1\n",
    "\n",
    "                if duplicate_id_e > 0:\n",
    "                    found_duplicates.append(duplicate_id_e)\n",
    "\n",
    "    return interleaved\n",
    "\n",
    "def get_softmax(ranking_indices,tau=3):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    numerator_list = [] #Numerator values for each of the ranked results\n",
    "    softmax_distribution = []\n",
    "\n",
    "    for rank_index in ranking_indices:\n",
    "        rank = rank_index + 1\n",
    "        numerator_value = 1/(rank**tau)\n",
    "        numerator_list.append(numerator_value)\n",
    "\n",
    "    denominator = sum(numerator_list)\n",
    "\n",
    "    for value in numerator_list:\n",
    "        probability = value/denominator\n",
    "        softmax_distribution.append(probability)\n",
    "\n",
    "    return softmax_distribution\n",
    "\n",
    "def prob_interleaving(ranking_pair,max_interleav=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    ranking_p = ranking_pair[0] #[(0,0),(0,0),(0,0)] form or duplicate [(0,1),(0,0),(0,0)]\n",
    "    ranking_e = ranking_pair[1] #[(0,0),(0,0),(0,0)] form or duplicate #[(0,1),(0,0),(0,0)]\n",
    "    interleaved = []\n",
    "    limit = len(ranking_p)\n",
    "\n",
    "    p_indices = list(range(limit))\n",
    "    e_indices = list(range(limit))\n",
    "\n",
    "    found_duplicates = []\n",
    "\n",
    "    while len(interleaved) < max_interleav:\n",
    "\n",
    "        p_priority = np.random.choice(2, 1)[0]\n",
    "\n",
    "        if (p_priority and len(p_indices) > 0) or len(e_indices) == 0:\n",
    "            softmax_p = get_softmax(p_indices)\n",
    "            doc_index_p = np.random.choice(p_indices, 1, p=softmax_p)[0]\n",
    "            p_indices.remove(doc_index_p)\n",
    "\n",
    "            result_p = ranking_p[doc_index_p]\n",
    "            relevance_p, duplicate_id_p = result_p\n",
    "\n",
    "            if duplicate_id_p == 0 :\n",
    "                interleaved.append((relevance_p, 0))\n",
    "            elif (duplicate_id_p > 0 and duplicate_id_p not in found_duplicates):\n",
    "                interleaved.append((relevance_p, 0))\n",
    "                found_duplicates.append(duplicate_id_p)\n",
    "                duplicate_index = ranking_e.index(result_p)\n",
    "                e_indices.remove(duplicate_index)\n",
    "        else:\n",
    "            softmax_e = get_softmax(e_indices)\n",
    "            doc_index_e = np.random.choice(e_indices, 1, p=softmax_e)[0]\n",
    "            e_indices.remove(doc_index_e)\n",
    "\n",
    "            result_e = ranking_e[doc_index_e]\n",
    "            relevance_e, duplicate_id_e = result_e\n",
    "\n",
    "            if duplicate_id_e == 0:\n",
    "                interleaved.append((relevance_e, 1))\n",
    "            elif (duplicate_id_e > 0 and duplicate_id_e not in found_duplicates):\n",
    "                interleaved.append((relevance_e, 1))\n",
    "                found_duplicates.append(duplicate_id_e)\n",
    "                duplicate_index = ranking_p.index(result_e)\n",
    "                p_indices.remove(duplicate_index)\n",
    "\n",
    "    return interleaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Documentation </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Generate Input </h3>\n",
    "<ul>\n",
    "<li>gen_input_unsorted(length, n) <br>\n",
    "    Creates a list of all possible combinations of relevance scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "        Length of a combination of relevance scores.\n",
    "    n : int\n",
    "        Maximum relevance score\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A list containing all possible combinations of relevance scores.\n",
    "    \n",
    "\n",
    "</li>\n",
    "\n",
    "<li>gen_input(length, n) <br>\n",
    "    Creates a sorted list of all possible combinations\n",
    "    of relevance scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "        Length of a combination of relevance scores.\n",
    "    n : int\n",
    "        Maximum relevance score\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A sorted list containing all possible combinations\n",
    "        of relevance scores.\n",
    "\n",
    "    \n",
    "</li>\n",
    "\n",
    "<li>     gen_input_pairs(length, n) <br>\n",
    "    Creates a sorted list of all possible pairs of combinations\n",
    "    of relevance scores.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    length : int\n",
    "        Length of a combination of relevance scores.\n",
    "    n : int\n",
    "        Maximum relevance score\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A sorted list containing all possible pairs of combinations\n",
    "        of relevance scores.\n",
    "</li>\n",
    "\n",
    "<li>get_conflicts(n, length, _in=[], ordered=False) <br>\n",
    "    Creates a list of all possible id conflicts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Total possible number of id conflicts at a time.\n",
    "    length: int\n",
    "        Length of the list containing id conflicts.\n",
    "    _in : array_like\n",
    "        Array of current id conflicts.\n",
    "    ordered : bool\n",
    "        Indicates whether id conflict numbers are to appear in order\n",
    "        or are permitted to appear in any order.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A list containing all possible id conflicts.\n",
    "        A value of 0 indicates no conflict,\n",
    "        a value higher than indicates a conflict.\n",
    "\n",
    "    \n",
    "</li>\n",
    "\n",
    "<li>add_conflicts(pair) <br>\n",
    "    Adds id conflicts to a tuple of combinations of relevance grades.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pair : tuple\n",
    "        Tuple of combinations of relevance grades.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : list\n",
    "        A list of all possible tuples of combinations\n",
    "        of relevance grades with id conflicts.\n",
    "        Relevance grades from the inputs have been replaced by a tuple\n",
    "        of relevance grade and id conflict number.\n",
    "    \n",
    "    \n",
    "</li>\n",
    "\n",
    "<li>ERR(g_list, R_func=lambda g, max_g: float(2**g- 1) / 2**max_g) <br>\n",
    "    Calculates Expected Reciprocal Rank for one list\n",
    "    of relevance grades.\n",
    "    \n",
    "    Source\n",
    "    ------\n",
    "    The algorithm originates from a paper by O. Chapelle et al.\n",
    "    named \"Expected Reciprocal Rank for Graded Relevance\"\n",
    "    and can be found here: http://olivier.chapelle.cc/pub/err.pdf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    g_list : array_like\n",
    "        Array of relevance grades.\n",
    "    R_func : function(int, int) -> float\n",
    "        Function that converts a relevance grade\n",
    "        to probability of relevance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ERR : float\n",
    "        Expected Reciprocal Rank.\n",
    "    \n",
    "    \n",
    "</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Interleaving </h3>\n",
    "<ul>\n",
    "<li>td_interleaving(ranking_par,max_interleav=3) <br>\n",
    "Run Team-draft interleaving given a ranking pair as input\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    An ranking pair : List of ranked and labeled results\n",
    "        Index of list elements represents the rank of that element. Each\n",
    "        element is a tuple of the relevance score and a duplicate ID.\n",
    "        If this value is 0, it has no duplicate with a document in the results of other ranker.\n",
    "        If the value is greater than 0, then it has a duplicate with another result of the other ranker that matches this number\n",
    "        No duplicate example: E ranked list:  [(0,0),(0,0),(0,0)] and P ranked list: [(1,0),(0,0),(0,0)] form\n",
    "        2 duplicates example: E ranked list: [(1,1),(0,2),(0,0)]  and P ranked list: [(1,0),(1,1),(0,2)]  for example\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Interleaved list (of length 3 as default) based on Team-draft method: list\n",
    "        Index + 1 represents the rank of the interleaved list and element is an tuple of the form (relevance: binary,ranker credit:binary), Credits are assigned as P(0) and E(1)\n",
    "</li>        \n",
    "        \n",
    "<li> get_softmax(ranking_indices,tau=3)<br>\n",
    "    Compute softmax distribution for a ranker given the indices of documents that are avaliable to be picked.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Ranking indices : List of indices that represent the rank of documents that can be picked for next interleaving.\n",
    "    Example of nothing picked of ranker: List of the form: [0,1,2]\n",
    "    Example of document already picked of ranker: List of the form: [0,2]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List of softmax probabilities for documents to be picked: List\n",
    "        The list maps one to one the ranking indices that were given as input, each element is a probability for the document for that particular index\n",
    "        in that ranking indices list, which in turn consist of numbers that represents the actual indices to be picked from ranked results.\n",
    "</li>\n",
    "\n",
    "<li> prob_interleaving(ranking_pair,max_interleav=3)<br>\n",
    "    Run Probabilistic interleaving  given a ranking pair as input\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    An ranking pair : List of ranked and labeled results\n",
    "        Index of list elements represents the rank of that element. Each\n",
    "        element is a tuple of the relevance score and a duplicate ID.\n",
    "        If this value is 0, it has no duplicate with a document in the results of other ranker.\n",
    "        If the value is greater than 0, then it has a duplicate with another result of the other ranker that matches this number\n",
    "        No duplicate example: E ranked list:  [(0,0),(0,0),(0,0)] and P ranked list: [(1,0),(0,0),(0,0)] form\n",
    "        2 duplicates example: E ranked list: [(1,1),(0,2),(0,0)]  and P ranked list: [(1,0),(1,1),(0,2)]  for example\n",
    "    Returns\n",
    "    -------\n",
    "    Interleaved list (of length 3 as default) based on probabilistc interleaving method: list\n",
    "        Index + 1 represents the rank of the interleaved list and element is an tuple of the form (relevance: binary,ranker credit:binary), credits are assigned as P(0) and E(1)\n",
    "    \n",
    "    \n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Analysis </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
